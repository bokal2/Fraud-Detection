{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from sklearn.metrics import f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def ShowStats(cmat, y_test, pred):\n",
    "   # separate out the confusion matrix components\n",
    "   tpos = cmat[0][0]\n",
    "   fneg = cmat[1][1]\n",
    "   fpos = cmat[0][1]\n",
    "   tneg = cmat[1][0]\n",
    "   # calculate F!, Recall scores\n",
    "   f1Score = round(f1_score(y_test, pred), 2)\n",
    "   recallScore = round(recall_score(y_test, pred), 2)\n",
    "   # calculate and display metrics\n",
    "   print(cmat)\n",
    "   print( 'Accuracy: '+ str(np.round(100*float(tpos+fneg)/float(tpos+fneg + fpos + tneg),2))+'%')\n",
    "   print( 'Cohen Kappa: '+ str(np.round(cohen_kappa_score(y_test, pred),3)))\n",
    "   print(\"Sensitivity/Recall for Model : {recall_score}\".format(recall_score = recallScore))\n",
    "   print(\"F1 Score for Model : {f1_score}\".format(f1_score = f1Score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel(model, X_train, y_train, X_test, y_test):\n",
    "   model.fit(X_train, y_train.values.ravel())\n",
    "   pred = model.predict(X_test)\n",
    "   matrix = confusion_matrix(y_test, pred)\n",
    "   return matrix, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Fraud    284315\n",
      "Fraud           492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/creditcard.csv')\n",
    "class_names = {0:'Not Fraud', 1:'Fraud'}\n",
    "print(df.Class.value_counts().rename(index = class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features that we want to use\n",
    "feature_names = df.iloc[:, 1:30].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
       "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
       "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.iloc[:1, 30: ].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Class'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = df[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_features,    data_target, train_size=0.70, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fidelity\\appdata\\local\\conda\\conda\\envs\\portal\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85293    15]\n",
      " [   57    78]]\n",
      "Accuracy: 99.92%\n",
      "Cohen Kappa: 0.684\n",
      "Sensitivity/Recall for Model : 0.58\n",
      "F1 Score for Model : 0.68\n"
     ]
    }
   ],
   "source": [
    "# Picking our machine learning technique or model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "cmat, pred = RunModel(lr, X_train, y_train, X_test, y_test)\n",
    "ShowStats(cmat, y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the accuracy was great, the algorithm misclassified more than 4 in 10 fraudulent transactions.\n",
    "So, since accuracy is not the reliable measure of our model’s effectiveness. Instead, we look at other measures like the Cohen Kappa, Recall, and F1 score and we should achieve a score as close to 1 as we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85296    12]\n",
      " [   30   105]]\n",
      "Accuracy: 99.95%\n",
      "Cohen Kappa: 0.833\n",
      "Sensitivity/Recall for Model : 0.78\n",
      "F1 Score for Model : 0.83\n"
     ]
    }
   ],
   "source": [
    "# Let's try another model. In this case; RandomForest classifier.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 100, n_jobs =4)\n",
    "cmat, pred = RunModel(rf, X_train, y_train, X_test, y_test)\n",
    "ShowStats(cmat, y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s quite a bit better. Note the accuracy went up slightly, but the other scores showed significant improvements as well. Therefore, Random Forest model performed better overally than Logistic Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
